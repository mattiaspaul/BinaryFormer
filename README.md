# BinaryFormer
MIDL2025 Submission on using Hamming distances in Self-Attention for Long Range Vision Transformers

## Overview
âš¡ï¸BinaryFormer aims to reduce computational and memory demand of long-range transformers by reducing the precision of the large matrix multiplication of queries and keys from floating point to binary. ðŸš€ It is also the first method to propose a binary backward computation making BinaryFormer models not only efficient at inference but also during training. Key to make this possible is a decomposition of QK^T into a non-differentiable Hamming distance and a (scalar) differentiable weighing based on the binarisation difference. ðŸŽ‰ This repository provides the following features:
- [x] Concept of binary in pytorch self-attention with custom backward path
- [x] Efficient CPU implementation using numpy-2.2's ``bitwise_count``  
- [x] Model variants: base, sign (adhoc STE), sign_sg (stop grad.), Hamming, HammingG (with group-linear), HammingB (w/o learned weights)
- [x] Experimental settings for pixel-transformer CIFAR, fine-tuning DINO ViT-S8 segmentation and 3D VQ-diffusion models 
- [x] MPSGraph (Apple GPU) ``HammingDistanceWithPrimaryTensor`` implementation of Hamming-Attention including custom bitpacking
- [ ] Todo: Triton/Cutlass kernels ``wmma_binary_gemm.cu`` that leverage TensorCores and deeper integration into pytorch

## Motivation
While weight quantisation for improved storage and inference of deep transformer models is commonplace, much resources are wasted to compute QK^T with floating point precision (Nvidias Transformer Engine / Hopper is specifically build to enable FP8 in this step). By enabling a binary computation in both forward and backward step theoretical speed-ups and efficiency gains of 16x are possible with little to no sacrifice in performance. This is particularly important for long-range transformers (e.g. in 3D medical imaging) with token lenghts of a thousand and more. The following chart 

## Abstract

Vision transformers have become essential for many medical image analysis tasks. They particular excel at capturing long-range interactions at the cost of a computational effort that grows quadratically with sequence length. This becomes even problematic for 3D problems, in particular for high-resolution diffusion models that require dozens of sampling steps. Flash attention successfully addressed the memory limitations of long sequences by optimising local memory access, but left the computational burden high. Quantising weights and activations for convolutions has been an active area of research for years, yielding completely binary networks that are however trained at higher precision and result in substantial performance drops. For transformers recent studies have been limited to quantising weights in linear layers or in an orthogonal research direction exploiting the potential of sparsity in self-attention scores. We present a novel scheme that not only enables a binary precision computation of the self-attention at inference time but even extends this to the training of transformers. To achieve differentiability we combine the bitwise Hamming distance with a learnable scalar query and key weighting. In theory this yields a 16-fold reduction in arithmetic operations and memory bandwidth - paving a way for more resource-efficient sequence modelling. We evaluate our model on three tasks with sequence lengths of N$>$1000, classification of images without patch-embedding, semantic 2D MRI segmentation and 3D diffusion models for inpainting and generative synthesise of high-resolution volumes. Our results demonstrate competitive performance and we provide an intuitive reasoning for the effectiveness of differentiable key-, query- weighting through Bernoulli sampling and high-dimensional interpolation.
